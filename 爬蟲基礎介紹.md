# ***WS-Python 爬蟲基礎教學***

### 我們今天以爬取台灣證券交易所裡個股日成交資訊為例

#### Requests 
- 我們這邊使用的模組，是用 `requests` (相關文件可以查詢python的api) 這邊簡單介紹大概的用法，幾乎很多爬蟲都可以使用。
- requests有兩種方式，一種是get、一種是post，這邊是使用get的方式
- 為什麼使用get呢，最明顯的判讀方式就是，選擇不同的瀏覽畫面時，他的url會長的不一樣
- 使用前必須先 `import requests`，語法如下:

`res = requests.get('https://www.twse.com.tw/exchangeReport/STOCK_DAY?response=json&date=20201107&stockNo=0050')`

- 可以看到網址列中 `date` 會等於一天的日期，而 `stockNo` 會等於其中一支股票的代碼，這些都是可以替換的

#### BeautifulSoup 
- 之後我們要整理requests下來的 html 
- python裡有一個非常好用的模組 `BeautifulSoup` (相關文件可以查詢python的api)
- 但BeautifulSoup不是預設的api，所以第一次使用時要將此api 下載下來，可以打開cmd視窗使用 `pip install BeautifulSoup`
- 使用前必須先 `from bs4 import BeautifulSoup` 呼叫他

`soup = BeautifulSoup(res.content, 'lxml')` <br> (lxml是beautifulSoup其中一個解析方式，詳細可搜尋相關文件)

#### 將 soup 整理成可分析的資料 
- 若 `print(soup.text)` 則可以看到類似於 `json` 的內容，我們可以利用 `json` 模組來處理 <br>
```
import json
data = json.loads(soup.text)
print(data)
```
- 我們可以將 `data` 整理並印出來看看
```
print(data['fields'])
for d in data['data']:
    print(d)
```
- 現在 `data` 裡儲存的就是已經整理好的字典檔案，後續可以將這些資料處理成 `csv` 或是 `xlsx` 檔案讓使用者去做分析處理


## 以上為簡單的爬蟲介紹，由於種類太多，還會有各式各樣的爬蟲會於github上po出程式碼範例
## 若有問題，可以洽詢 LINE: @mumustudio
